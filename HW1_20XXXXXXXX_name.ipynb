{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #1 \n",
    "\n",
    "#### Machine Learning in Korea University\n",
    "#### COSE362, Fall 2018 (Prof. Jaewoo Kang)\n",
    "#### Due : 11/6 (TUE) 11:59 PM\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this assignment, you will learn model selection process among various hyperparameters.\n",
    "* Implementation detail: Anaconda 5.3 with python 3.7\n",
    "* Use given dataset. Please do not change training / validation / test split.\n",
    "* Use numpy, scikit-learn, and matplotlib library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Regression with Feature Selection\n",
    "In this example we will conduct featrue selection process in linear regression model. <br>\n",
    "You will use data in 'LinReg' directory for this example. <br>\n",
    "Please perform the following steps. \n",
    "> 0. Preprocess: Change given dataset into input array for scikit-learn model.\n",
    "> 1. Feture selection : perform greedy feature selection.\n",
    "> 2. Plot: plot validation and train error against number of feature.\n",
    "> 3. Model selection and evaluation: Select best model and perform evaluation on test dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-0. Preprocess\n",
    "Load dataset and process it into appropriate array form.\n",
    "* Example <br>\n",
    "> For linear regression problem, the datasets are described onto 'dev_sample.npy', 'dev_label.npy', 'test_sample.npy', 'test_label.npy' in 'LinReg' folder. <br>\n",
    "> Load these datasets onto <b>X_dev, y_dev, X_test, y_test</b>. <br>\n",
    "> You may need to use numpy.load function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load linear regression datasets\n",
    "# Your code here\n",
    "\n",
    "# End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. Feature selection\n",
    "Build linear regression models with different number of features. (1 ~ 100)<br>\n",
    "Please use <b>cross validation</b>, <b>greedy approach</b> for feature selection until choose optimal number of features. <br> \n",
    "\n",
    "* For cross validaton, you need to split your development set into 5-fold. This is implemented into class <b>cv</b>.\n",
    "* Feature selection example : Input with 10 features\n",
    "> Call 10 features as #1, #2, #3, ..., #10 <br>\n",
    "> First build 10 models with only one feature. \n",
    "> Compare model with #1, model with #2, ... , model with #10 <br>\n",
    "> Choose feature of the best model. (for example, #1 is the best) <br>\n",
    "> Build model with 2 features. (#1, #2), (#1, #3), ..., (#1, #10). <br>\n",
    "> Then, add feature with the best performance. <br>\n",
    "> And so on...\n",
    "\n",
    "<b>For the next step, please save validation and train error of the best model for each number of selected features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define linear regression function\n",
    "# You may use sklearn.linear_model.LinearRegression\n",
    "# Your code here\n",
    "\n",
    "# End your code\n",
    "\n",
    "# Basic settings. DO NOT MODIFY\n",
    "selected_feature = []\n",
    "sel_num = 100\n",
    "valid_split = 1/5\n",
    "cv = ShuffleSplit(n_splits=5, test_size=valid_split, random_state=0)\n",
    "\n",
    "selected_train_error = []\n",
    "selected_valid_error = []\n",
    "\n",
    "# For greedy selection\n",
    "for sel in range(sel_num) :\n",
    "    min_train_error = +1000\n",
    "    min_valid_error = +1000\n",
    "    min_feature = 0\n",
    "    \n",
    "    # For each feature\n",
    "    for i in range(X_dev.shape[1]) :\n",
    "        train_error_ith = []\n",
    "        valid_error_ith = []\n",
    "        \n",
    "        # Select feature greedy\n",
    "        # Hint : There should be no duplicated feature in selected_feature\n",
    "        # Your code here\n",
    "\n",
    "        # End your code\n",
    "        \n",
    "        # For cross validation\n",
    "        for train_index, test_index in cv.split(X_dev) :\n",
    "            X_train, X_valid = X_dev_fs[train_index], X_dev_fs[test_index]\n",
    "            y_train, y_valid = y_dev[train_index], y_dev[test_index]\n",
    "        \n",
    "            # Derive training error, validation error\n",
    "            # You may use sklearn.metrics.mean_squared_error, model.fit(), model.predict()\n",
    "            # Your code here\n",
    "\n",
    "            # End your code\n",
    "            \n",
    "        # Select best performance feature set on each features\n",
    "        # You should choose the feature which has minimum mean cross validation error\n",
    "        # Your code here\n",
    "\n",
    "        # End your code\n",
    "\n",
    "    print('='*50)\n",
    "    print(\"# of selected feature(s) : {}\".format(sel+1))\n",
    "    print(\"Selected feature of this iteration : {}\".format(min_feature))\n",
    "    selected_feature.append(min_feature)\n",
    "    selected_train_error.append(min_train_error)\n",
    "    selected_valid_error.append(min_valid_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. Plot error\n",
    "Plot train and validation error against number of features.<br>\n",
    "After plotting, <b>analyze the result graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train error plot\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(np.arange(1,sel_num+1), selected_train_error)\n",
    "plt.title('Training error')\n",
    "plt.legend(['error'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation error plot\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(np.arange(1,sel_num+1), selected_valid_error)\n",
    "plt.title('Validation error')\n",
    "plt.legend(['error'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze\n",
    "Write explanation of graph below. <br>\n",
    "Analyze the folloing points.\n",
    "* Trend of each error against number of features\n",
    "* Meaning of gap between vlidation error and train error\n",
    "* Meaning of each region in graph\n",
    "* Others..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write description here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. Model selection and evaluation\n",
    "Select the best model and perform a test on test dataset.<br>\n",
    "Print the <b>performance on test set</b> with <b>features of the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select optimal feature set corresponding the minimum cross validation error\n",
    "# Your code here\n",
    "\n",
    "# End your code\n",
    "\n",
    "# Basic settings. DO NOT MODIFY\n",
    "min_train_error = 1000\n",
    "min_valid_error = 1000\n",
    "optimal_param = np.array([])\n",
    "\n",
    "for train_index, test_index in cv.split(X_dev) :\n",
    "    X_train, X_valid = X_dev_fs[train_index], X_dev_fs[test_index]\n",
    "    y_train, y_valid = y_dev[train_index], y_dev[test_index]\n",
    "    \n",
    "    # Derive training error, validation error for each fold\n",
    "    # For each fold, you need to compare error with previous minimum error.\n",
    "    # Your code here\n",
    "\n",
    "    # End your code\n",
    "\n",
    "# Find the best model on each fold\n",
    "# Derive test error with best performance model\n",
    "# Your code here\n",
    "\n",
    "# End your code\n",
    "\n",
    "# Drop features of final model\n",
    "print(\"Results\")\n",
    "print(\"# of selected features : {}\".format(len(selected_feature)))\n",
    "print(\"Selected features : \")\n",
    "print(selected_feature)\n",
    "\n",
    "# Drop test error and accuracy\n",
    "print(\"Training error : {}\".format(min_train_error))\n",
    "print(\"Validation error : {}\".format(min_valid_error))\n",
    "print(\"Test error : {}\".format(test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression with Regularizer\n",
    "\n",
    "In this example you will explore the effect of regularization parameter.<br>\n",
    "You will use <b>'Heart Disease Dataset'</b> in <b>'LogReg'</b> for this example. <br>\n",
    "\n",
    "The goal is to predict the presence of heart disease given attributes of a patient.<br>\n",
    "The presence is integer valued from 0 (no presence) to 4, but you have to only distingush presensence (values 1,2,3,4) from absence (value 0). <br>\n",
    "Each attribute is described below. <br>\n",
    "\n",
    "> 1. age : age in years <br>\n",
    "> 2. sex : sex (1 = male; 0 = female) <br>\n",
    "> 3. cp : chest pain type <br>\n",
    "-- Value 1: typical angina <br>\n",
    "-- Value 2: atypical angina <br>\n",
    "-- Value 3: non-anginal pain <br>\n",
    "-- Value 4: asymptomatic  <br>\n",
    "> 4. trestbps : resting blood pressure (in mm Hg on admission to the hospital)  <br>\n",
    "> 5. chol : serum cholestoral in mg/dl  <br>\n",
    "> 6. fbs : (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false) <br>\n",
    "> 7. restecg  : resting electrocardiographic results <br>\n",
    "-- Value 0: normal <br>\n",
    "-- Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV) <br>\n",
    "-- Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria <br>\n",
    "> 8. thalach : maximum heart rate achieved <br>\n",
    "> 9. exang : exercise induced angina (1 = yes; 0 = no) <br>\n",
    "> 10. oldpeak : ST depression induced by exercise relative to rest <br>\n",
    "> 11. slope : the slope of the peak exercise ST segment <br>\n",
    "-- Value 1: upsloping <br>\n",
    "-- Value 2: flat <br>\n",
    "-- Value 3: downsloping  <br>\n",
    "> 12. ca : number of major vessels (0-3) colored by flourosopy  <br>\n",
    "> 13. thal : 3 = normal; 6 = fixed defect; 7 = reversable defect  <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-0. Preprocess\n",
    "\n",
    "Firstly, read training, validation and test datasets respectively. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_data(data_type):\n",
    "    f = open('./LogReg/' + data_type + '.data', 'r')\n",
    "\n",
    "    X, Y = [],[]\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line: break\n",
    "        spl = line.split(',')\n",
    "        x = spl[:-1]\n",
    "        y = int(spl[-1])\n",
    "        \n",
    "        X.append(list(map(float, x)))\n",
    "        \n",
    "        # Define the variable 'binary_label'.\n",
    "        # Note that labels must be 1 or 0.\n",
    "        # Your code here\n",
    "        \n",
    "        Y.append(binary_label)  # blank\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "X_tr, Y_tr = read_data('train')\n",
    "X_va, Y_va = read_data('valid')\n",
    "X_te, Y_te = read_data('test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization and Converting to one-hot vector\n",
    "\n",
    "Data preprocessing takes several steps after loading data. <br>\n",
    "1. <b>Normailze</b> numerical values. Normalization is defined as <b><i>normalized_value</i> = (value - mean) / std</b>. <br>\n",
    "   You should calculate mean and standard deviation (std) on <b> train data </b> and normalize train, valid and test data.\n",
    "2. For categorical attributes, <b>build dictionaries</b> of each attribute and convert the categorical values to <b>one-hot vectors</b>. <br>\n",
    "3. Concatenate all the obtained values. <br>\n",
    "\n",
    "If you have done correctly, you will get results that are same format as below: \n",
    "* <b>before</b> : [63.0, 1.0, 1.0, 145.0, 233.0, 1.0, 2.0, 150.0, 0.0, 2.3, 3.0, 0.0, 6.0]\n",
    "* <b>after</b> : [0.11099784710934087, 0, 1, 1, 0, 0, 0, 0.035386000081823056, -0.005256085700922788, 0, 1, 0, 0, 1, 0.0026598418293161848, 1, 0, 0.6659671864819814, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0] <br>\n",
    "(The values in the above example can be different from actual values.)<br>\n",
    "\n",
    "<b>Do not use any library such as sklearn.preprocessing. You can use only Numpy. </b><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Your code here\n",
    "\n",
    "# End your code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. Logistic regression model and regularizer\n",
    "Build logistic regression model with l2 regularization utilizing sklearn. <br>\n",
    "Find the optimal coefficient based on <b>cross entropy loss</b> on thet validation set. <br>\n",
    "Try following regularization parameter settings.\n",
    "* Regularization parameters = 0.01, 0.05, 0.1, 0.5, 1, 10, 100 <br>\n",
    "* Note that regluarization parameter for LogisticRegression in sklearn is inverse of true parameter. <br>\n",
    "  (coef = 0.001 for LogisticRegression   =>  $\\lambda$ = 1000 in our course note)\n",
    "* Your model should be <b>LogisticRegression(C=coef, solver='lbfgs', max_iter=500). </b>\n",
    "  <br>  <b>Do not change the model setting except C. </b> \n",
    "  <br> (coef = each regularization parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please use below function\n",
    "# logreg = LogisticRegression(C=coef, solver='lbfgs', max_iter=500)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "coefs = [0.01, 0.05, 0.1, 0.5, 1, 10, 100]\n",
    "\n",
    "opt_coef = 1\n",
    "\n",
    "\n",
    "# To plot losses on training and validation sets with varied parameter settings, \n",
    "# save them on lists.\n",
    "loss_tr, loss_va = [],[]\n",
    "\n",
    "# Your code here\n",
    "\n",
    "# End your code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. Plot error\n",
    "Plot the train and validation loss against given regularization parameter <b>(not inverse)</b>.<br>\n",
    "<b> Analyze the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not fix the code!!\n",
    "\n",
    "plt.plot(coefs, loss_tr, coefs, loss_va, 'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze \n",
    "Write explanation of graph below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. Model selection and evaluation\n",
    "\n",
    "Drop the performance on test set with the regularization coefficient of the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Your code here\n",
    "\n",
    "# End your code\n",
    "\n",
    "\n",
    "#print regularization paramter of final model and drop test loss and accuracy\n",
    "print (\"Optimal : {}, Loss : {:2.3f}, Accuracy : {:3.2f}\".format(coef, test_loss, test_acc*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
