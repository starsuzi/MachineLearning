{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2\n",
    "\n",
    "#### Machine Learning in Korea University\n",
    "#### COSE362, Fall 2018\n",
    "#### Due : 11/26 (TUE) 11:59 PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this assignment, you will learn various classification methods with given datasets.\n",
    "* Implementation detail: Anaconda 5.3 with python 3.7\n",
    "* Use given dataset. Please do not change train / valid / test split.\n",
    "* Use numpy, scikit-learn, and matplotlib library\n",
    "* You don't have to use all imported packages below. (some are optional). <br>\n",
    "Also, you can import additional packages in \"(Option) Other Classifiers\" part. \n",
    "* <b>*DO NOT MODIFY OTHER PARTS OF CODES EXCEPT \"Your Code Here\"*</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic packages\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Additional packages\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your own packages if you need(only in scikit-learn, numpy, pandas).\n",
    "# Your Code Here\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "#warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\"\n",
    "#End Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process\n",
    "\n",
    "> 1. Load \"train.csv\". It includes all samples' features and labels.\n",
    "> 2. Training four types of classifiers(logistic regression, decision tree, random forest, support vector machine) and <b>validate</b> it in your own way. <b>(You can't get full credit if you don't conduct validation)</b>\n",
    "> 3. Optionally, if you would train your own classifier(e.g. ensembling or gradient boosting), you can evaluate your own model on the development data. <br>\n",
    "> 4. <b>You should submit your predicted results on test data with the selected classifier in your own manner.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task & dataset description\n",
    "1. 6 Features (1~6)<br>\n",
    "Feature 2, 4, 6 : Real-valued<br>\n",
    "Feature 1, 3, 5 : Categorical <br>\n",
    "\n",
    "2. Samples <br>\n",
    ">In development set : 2,000 samples <br>\n",
    ">In test set : 1,500 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load development dataset\n",
    "Load your development dataset. You should read <b>\"train.csv\"</b>. This is a classification task, and you need to preprocess your data for training your model. <br>\n",
    "> You need to use <b>1-of-K coding scheme</b>, to convert categorical features to one-hot vector. <br>\n",
    "> For example, if there are 3 categorical values, you can convert these features as [1,0,0], [0,1,0], [0,0,1] by 1-of-K coding scheme. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training your model, you need to convert categorical features to one-hot encoding vectors.\n",
    "# Your Code Here\n",
    "#load data\n",
    "df_train = pd.read_csv('./data/train.csv')\n",
    "\n",
    "#one-hot\n",
    "df_train_onehot = pd.get_dummies(df_train.drop(columns=['feature2','feature4','feature6','target']))\n",
    "df_train = pd.concat([df_train,df_train_onehot], axis=1)\n",
    "df_train.drop(['feature1','feature3','feature5'], axis = 1, inplace = True)\n",
    "\n",
    "#feature && label\n",
    "data = df_train.drop(['target'],axis = 1,inplace = False)\n",
    "target = df_train.target\n",
    "\n",
    "# End Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['feature2', 'feature4', 'feature6', 'target', 'feature1_a',\n",
       "       'feature1_b', 'feature1_c', 'feature1_d', 'feature3_a', 'feature3_b',\n",
       "       'feature3_c', 'feature3_d', 'feature3_e', 'feature3_f', 'feature3_g',\n",
       "       'feature3_h', 'feature5_a', 'feature5_b', 'feature5_c', 'feature5_d',\n",
       "       'feature5_e', 'feature5_f', 'feature5_g', 'feature5_h'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 811,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature6</th>\n",
       "      <th>target</th>\n",
       "      <th>feature1_a</th>\n",
       "      <th>feature1_b</th>\n",
       "      <th>feature1_c</th>\n",
       "      <th>feature1_d</th>\n",
       "      <th>feature3_a</th>\n",
       "      <th>feature3_b</th>\n",
       "      <th>...</th>\n",
       "      <th>feature3_g</th>\n",
       "      <th>feature3_h</th>\n",
       "      <th>feature5_a</th>\n",
       "      <th>feature5_b</th>\n",
       "      <th>feature5_c</th>\n",
       "      <th>feature5_d</th>\n",
       "      <th>feature5_e</th>\n",
       "      <th>feature5_f</th>\n",
       "      <th>feature5_g</th>\n",
       "      <th>feature5_h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature2  feature4  feature6  target  feature1_a  feature1_b  feature1_c  \\\n",
       "0         1         5         2       3           1           0           0   \n",
       "1         2         8         2      15           0           1           0   \n",
       "2         1         3         7       6           0           1           0   \n",
       "3         1         5         7      13           0           0           1   \n",
       "4         3         2         4      15           0           0           0   \n",
       "\n",
       "   feature1_d  feature3_a  feature3_b     ...      feature3_g  feature3_h  \\\n",
       "0           0           0           0     ...               0           0   \n",
       "1           0           1           0     ...               0           0   \n",
       "2           0           0           0     ...               0           0   \n",
       "3           0           1           0     ...               0           0   \n",
       "4           1           0           0     ...               0           1   \n",
       "\n",
       "   feature5_a  feature5_b  feature5_c  feature5_d  feature5_e  feature5_f  \\\n",
       "0           0           0           0           1           0           0   \n",
       "1           0           0           0           0           1           0   \n",
       "2           0           0           0           0           0           0   \n",
       "3           0           0           0           0           0           0   \n",
       "4           0           0           0           0           0           0   \n",
       "\n",
       "   feature5_g  feature5_h  \n",
       "0           0           0  \n",
       "1           0           0  \n",
       "2           0           1  \n",
       "3           0           1  \n",
       "4           1           0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 812,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['feature2', 'feature4', 'feature6', 'feature1_a', 'feature1_b',\n",
       "       'feature1_c', 'feature1_d', 'feature3_a', 'feature3_b', 'feature3_c',\n",
       "       'feature3_d', 'feature3_e', 'feature3_f', 'feature3_g', 'feature3_h',\n",
       "       'feature5_a', 'feature5_b', 'feature5_c', 'feature5_d', 'feature5_e',\n",
       "       'feature5_f', 'feature5_g', 'feature5_h'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 813,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "test = SelectKBest(score_func=chi2, k=4)\n",
    "fit = test.fit(data, target)\n",
    "features = fit.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[261.042  32.881 463.79  310.517  82.854  69.295 152.994  47.463  30.991\n",
      "  48.901  15.011  27.886  25.868  51.347  12.539 173.214  52.463  50.316\n",
      "  72.091  71.496  22.83   27.619 107.541]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "print(fit.scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_train.drop(['feature3_d', 'feature3_g','feature5_f', 'feature4'],axis = 1,inplace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.109 0.238 0.214 0.01  0.013 0.02  0.022 0.029 0.029 0.023 0.028 0.033\n",
      " 0.03  0.028 0.027 0.017 0.018 0.014 0.016 0.015 0.022 0.022 0.023]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "# fit an Extra Trees model to the data\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(data,target)\n",
    "# display the relative importance of each attribute\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_train.drop(['feature2'],axis = 1,inplace = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "Train and validate your <b>logistic regression classifier</b>, and print out your validation(or cross-validation) error.\n",
    "> If you want, you can use cross validation, regularization, or feature selection methods. <br>\n",
    "> <b> You should use F1 score('macro' option) as evaluation metric. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import expon\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "clf = LogisticRegression()\n",
    "C_distr = expon(scale=2)\n",
    "param_grid_random = {'C': C_distr, 'penalty': ['l1', 'l2']}\n",
    "\n",
    "randomsearch = RandomizedSearchCV(clf, param_grid_random, n_iter=5)\n",
    "\n",
    "randomsearch.fit(data, target);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 2.511835498621428, 'penalty': 'l1'}"
      ]
     },
     "execution_count": 817,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:    0.3 | mean(train_error):  1.9364 | mean(val_error): 2.3035 | f1_score: 0.5415307540051867\n",
      "C:    0.6 | mean(train_error):  1.8997 | mean(val_error): 2.1815 | f1_score: 0.5401291946933984\n",
      "C:    2.5 | mean(train_error):  1.9105 | mean(val_error): 2.313 | f1_score: 0.5734321108169915\n",
      "C:      2 | mean(train_error):  1.9037 | mean(val_error): 2.3195 | f1_score: 0.5734321108169915\n",
      "C:    6.8 | mean(train_error):  1.9339 | mean(val_error): 2.2905 | f1_score: 0.570977185102572\n",
      "C:      4 | mean(train_error):  1.9233 | mean(val_error): 2.342 | f1_score: 0.570977185102572\n",
      "C:      5 | mean(train_error):  1.9314 | mean(val_error): 2.3055 | f1_score: 0.570977185102572\n",
      "C:     10 | mean(train_error):  1.9468 | mean(val_error): 2.3175 | f1_score: 0.5854215060350962\n"
     ]
    }
   ],
   "source": [
    "# Training your logistic regression classifier, and print out your validation(or cross-validation) error.\n",
    "# Save your own model\n",
    "# Your Code Here\n",
    "\n",
    "#scores_accuracy = cross_val_score(logreg, X, y, cv = 10, scoring = 'accuracy')\n",
    "#scores_f1 = cross_val_score(logreg, X, y, cv = 10, scoring = 'f1_macro')\n",
    "#print(scores_f1)\n",
    "\n",
    "def calc_train_error(X_train, y_train, model):\n",
    "    #returns in-sample error for already fot model\n",
    "    predictions = model.predict(X_train)\n",
    "    mse = mean_squared_error(y_train, predictions)\n",
    "    return mse\n",
    "\n",
    "def calc_test_error(X_test, y_test, model):\n",
    "    #returns out-of-sample error for already fit model\n",
    "    predictions = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions, average='macro')\n",
    "    #print(accuracy_score(y_test, predictions))\n",
    "    return mse, f1\n",
    "    \n",
    "def calc_metrics(X_train, y_train, X_test, y_test, model):\n",
    "    #fits model and returns MSE for in-sample-error and out-of-sample error\n",
    "    model.fit(X_train, y_train)\n",
    "    train_error = calc_train_error(X_train, y_train, model)\n",
    "    val_error, f1 = calc_test_error(X_test, y_test, model)\n",
    "    return train_error, val_error, f1\n",
    "\n",
    "#Create Pipeline\n",
    "standardizer = StandardScaler()\n",
    "\n",
    "#Create K-fold cross-validation\n",
    "alphas = [0.3,0.6,2.5,2,6.8,4,5,10]\n",
    "#smaller values specify stronger regularization\n",
    "#C가 작을수록 강한 resularization -> flat해짐\n",
    "#C가 클수록 overfit\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "for alpha in alphas:\n",
    "    train_errors = []\n",
    "    val_errors = []\n",
    "    for train_index, val_index in kf.split(data, target):\n",
    "        \n",
    "        #split data\n",
    "        X_train, X_val = data.iloc[train_index], data.iloc[val_index]\n",
    "        y_train, y_val = target.iloc[train_index], target.iloc[val_index]\n",
    "        \n",
    "        #instantaite model\n",
    "        #Create a pipleline that standardized, then runs logistic regression\n",
    "        pipeline = make_pipeline(standardizer,\n",
    "                                 LogisticRegression(C = alpha, penalty='l1'))\n",
    "        \n",
    "        #calculate errors\n",
    "        train_error, val_error, f1score = calc_metrics(X_train, y_train, X_val, y_val, pipeline)\n",
    "        \n",
    "        #append tp appropriate list\n",
    "        train_errors.append(train_error)\n",
    "        val_errors.append(val_error)\n",
    "    \n",
    "    #generate report\n",
    "    print('C: {:6} | mean(train_error): {:7} | mean(val_error): {} | f1_score: {}'.\n",
    "          format(alpha,\n",
    "                 round(np.mean(train_errors),4),\n",
    "                 round(np.mean(val_errors),4),\n",
    "                 f1score\n",
    "                ))\n",
    "\n",
    "#val_errors_logreg.append(mean_squared_error(regr.predict(X_valid), y_valid))\n",
    "\n",
    "# End Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "Train and validate your <b>decision tree classifier</b>, and print out your validation(or cross-validation) error.\n",
    "> If you want, you can use cross validation, regularization, or feature selection methods. <br>\n",
    "> <b> You should use F1 score('macro' option) as evaluation metric. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth:     10 | mean(train_error):     0.0 | mean(val_error): 0.0 | f1_score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Training your decision tree classifier, and print out your validation(or cross-validation) error.\n",
    "# Save your own model\n",
    "# Your Code Here\n",
    "\n",
    "#Create Pipeline\n",
    "standardizer = StandardScaler()\n",
    "\n",
    "#Create K-fold cross-validation\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "train_errors = []\n",
    "val_errors = []\n",
    "for train_index, val_index in kf.split(data, target):       \n",
    "    #split data\n",
    "    X_train, X_val = data.iloc[train_index], data.iloc[val_index]\n",
    "    y_train, y_val = target.iloc[train_index], target.iloc[val_index]\n",
    "        \n",
    "    #instantaite model\n",
    "    #Create a pipleline that standardized, then runs logistic regression\n",
    "    pipeline = make_pipeline(standardizer,\n",
    "                            DecisionTreeClassifier())\n",
    "        \n",
    "    #calculate errors\n",
    "    train_error, val_error, f1score = calc_metrics(X_train, y_train, X_val, y_val, pipeline)\n",
    "        \n",
    "    #append tp appropriate list\n",
    "    train_errors.append(train_error)\n",
    "    val_errors.append(val_error)\n",
    "    \n",
    "#generate report\n",
    "print('depth: {:6} | mean(train_error): {:7} | mean(val_error): {} | f1_score: {}'.\n",
    "        format(depth,\n",
    "               round(np.mean(train_errors),4),\n",
    "               round(np.mean(val_errors),4),\n",
    "               f1score\n",
    "                ))\n",
    "\n",
    "# End Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "Train and validate your <b>random forest classifier</b>, and print out your validation(or cross-validation) error.\n",
    "> If you want, you can use cross validation, regularization, or feature selection methods. <br>\n",
    "> <b> You should use F1 score('macro' option) as evaluation metric. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth:     10 | mean(train_error):  0.0168 | mean(val_error): 2.2195 | f1_score: 0.7704807114355763\n"
     ]
    }
   ],
   "source": [
    "# Training your random forest classifier, and print out your validation(or cross-validation) error.\n",
    "# Save your own model\n",
    "# Your Code Here\n",
    "#Create Pipeline\n",
    "standardizer = StandardScaler()\n",
    "\n",
    "#Create K-fold cross-validation\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "train_errors = []\n",
    "val_errors = []\n",
    "for train_index, val_index in kf.split(data, target):       \n",
    "    #split data\n",
    "    X_train, X_val = data.iloc[train_index], data.iloc[val_index]\n",
    "    y_train, y_val = target.iloc[train_index], target.iloc[val_index]\n",
    "        \n",
    "    #instantaite model\n",
    "    #Create a pipleline that standardized, then runs logistic regression\n",
    "    pipeline = make_pipeline(standardizer,\n",
    "                            RandomForestClassifier())\n",
    "        \n",
    "    #calculate errors\n",
    "    train_error, val_error, f1score = calc_metrics(X_train, y_train, X_val, y_val, pipeline)\n",
    "        \n",
    "    #append tp appropriate list\n",
    "    train_errors.append(train_error)\n",
    "    val_errors.append(val_error)\n",
    "    \n",
    "#generate report\n",
    "print('depth: {:6} | mean(train_error): {:7} | mean(val_error): {} | f1_score: {}'.\n",
    "        format(depth,\n",
    "               round(np.mean(train_errors),4),\n",
    "               round(np.mean(val_errors),4),\n",
    "               f1score\n",
    "                ))\n",
    "\n",
    "# End Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine\n",
    "Train and validate your <b>support vector machine classifier</b>, and print out your validation(or cross-validation) error.\n",
    "> If you want, you can use cross validation, regularization, or feature selection methods. <br>\n",
    "> <b> You should use F1 score('macro' option) as evaluation metric. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from sklearn import svm,grid_search\n",
    "def svc_param_selection(X, y, nfolds):\n",
    "    Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "    gammas = [0.001, 0.01, 0.1, 1]\n",
    "    param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "    grid_search = GridSearchCV(svm.SVC(kernel='rbf'), param_grid, cv=nfolds)\n",
    "    grid_search.fit(X, y)\n",
    "    grid_search.best_params_\n",
    "    return grid_search.best_params_\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 25, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.010 (+/-0.001) for {'C': 0.001, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.010 (+/-0.001) for {'C': 0.001, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.010 (+/-0.001) for {'C': 0.001, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.010 (+/-0.001) for {'C': 0.001, 'gamma': 1e-05, 'kernel': 'rbf'}\n",
      "0.203 (+/-0.023) for {'C': 0.1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.099 (+/-0.010) for {'C': 0.1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.010 (+/-0.001) for {'C': 0.1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.010 (+/-0.001) for {'C': 0.1, 'gamma': 1e-05, 'kernel': 'rbf'}\n",
      "0.203 (+/-0.023) for {'C': 0.1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.099 (+/-0.010) for {'C': 0.1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.010 (+/-0.001) for {'C': 0.1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.010 (+/-0.001) for {'C': 0.1, 'gamma': 1e-05, 'kernel': 'rbf'}\n",
      "0.945 (+/-0.055) for {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.630 (+/-0.098) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.246 (+/-0.027) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.099 (+/-0.010) for {'C': 10, 'gamma': 1e-05, 'kernel': 'rbf'}\n",
      "1.000 (+/-0.000) for {'C': 25, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.821 (+/-0.122) for {'C': 25, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.385 (+/-0.037) for {'C': 25, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.076 (+/-0.009) for {'C': 25, 'gamma': 1e-05, 'kernel': 'rbf'}\n",
      "1.000 (+/-0.000) for {'C': 50, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.851 (+/-0.092) for {'C': 50, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.560 (+/-0.100) for {'C': 50, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.131 (+/-0.050) for {'C': 50, 'gamma': 1e-05, 'kernel': 'rbf'}\n",
      "1.000 (+/-0.000) for {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.945 (+/-0.055) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.630 (+/-0.098) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.247 (+/-0.027) for {'C': 100, 'gamma': 1e-05, 'kernel': 'rbf'}\n",
      "1.000 (+/-0.000) for {'C': 1000, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.986 (+/-0.056) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.945 (+/-0.055) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.630 (+/-0.098) for {'C': 1000, 'gamma': 1e-05, 'kernel': 'rbf'}\n",
      "0.010 (+/-0.001) for {'C': 0.001, 'gamma': 0.01, 'kernel': 'sigmoid'}\n",
      "0.010 (+/-0.001) for {'C': 0.001, 'gamma': 0.001, 'kernel': 'sigmoid'}\n",
      "0.010 (+/-0.001) for {'C': 0.001, 'gamma': 0.0001, 'kernel': 'sigmoid'}\n",
      "0.010 (+/-0.001) for {'C': 0.001, 'gamma': 1e-05, 'kernel': 'sigmoid'}\n",
      "0.032 (+/-0.003) for {'C': 0.1, 'gamma': 0.01, 'kernel': 'sigmoid'}\n",
      "0.044 (+/-0.005) for {'C': 0.1, 'gamma': 0.001, 'kernel': 'sigmoid'}\n",
      "0.010 (+/-0.001) for {'C': 0.1, 'gamma': 0.0001, 'kernel': 'sigmoid'}\n",
      "0.010 (+/-0.001) for {'C': 0.1, 'gamma': 1e-05, 'kernel': 'sigmoid'}\n",
      "0.032 (+/-0.003) for {'C': 0.1, 'gamma': 0.01, 'kernel': 'sigmoid'}\n",
      "0.044 (+/-0.005) for {'C': 0.1, 'gamma': 0.001, 'kernel': 'sigmoid'}\n",
      "0.010 (+/-0.001) for {'C': 0.1, 'gamma': 0.0001, 'kernel': 'sigmoid'}\n",
      "0.010 (+/-0.001) for {'C': 0.1, 'gamma': 1e-05, 'kernel': 'sigmoid'}\n",
      "0.194 (+/-0.026) for {'C': 10, 'gamma': 0.01, 'kernel': 'sigmoid'}\n",
      "0.554 (+/-0.100) for {'C': 10, 'gamma': 0.001, 'kernel': 'sigmoid'}\n",
      "0.128 (+/-0.052) for {'C': 10, 'gamma': 0.0001, 'kernel': 'sigmoid'}\n",
      "0.040 (+/-0.005) for {'C': 10, 'gamma': 1e-05, 'kernel': 'sigmoid'}\n",
      "0.209 (+/-0.052) for {'C': 25, 'gamma': 0.01, 'kernel': 'sigmoid'}\n",
      "0.681 (+/-0.135) for {'C': 25, 'gamma': 0.001, 'kernel': 'sigmoid'}\n",
      "0.300 (+/-0.067) for {'C': 25, 'gamma': 0.0001, 'kernel': 'sigmoid'}\n",
      "0.043 (+/-0.005) for {'C': 25, 'gamma': 1e-05, 'kernel': 'sigmoid'}\n",
      "0.207 (+/-0.065) for {'C': 50, 'gamma': 0.01, 'kernel': 'sigmoid'}\n",
      "0.822 (+/-0.123) for {'C': 50, 'gamma': 0.001, 'kernel': 'sigmoid'}\n",
      "0.385 (+/-0.037) for {'C': 50, 'gamma': 0.0001, 'kernel': 'sigmoid'}\n",
      "0.076 (+/-0.009) for {'C': 50, 'gamma': 1e-05, 'kernel': 'sigmoid'}\n",
      "0.192 (+/-0.033) for {'C': 100, 'gamma': 0.01, 'kernel': 'sigmoid'}\n",
      "0.862 (+/-0.087) for {'C': 100, 'gamma': 0.001, 'kernel': 'sigmoid'}\n",
      "0.560 (+/-0.100) for {'C': 100, 'gamma': 0.0001, 'kernel': 'sigmoid'}\n",
      "0.131 (+/-0.050) for {'C': 100, 'gamma': 1e-05, 'kernel': 'sigmoid'}\n",
      "0.171 (+/-0.024) for {'C': 1000, 'gamma': 0.01, 'kernel': 'sigmoid'}\n",
      "0.986 (+/-0.056) for {'C': 1000, 'gamma': 0.001, 'kernel': 'sigmoid'}\n",
      "0.851 (+/-0.092) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'sigmoid'}\n",
      "0.560 (+/-0.100) for {'C': 1000, 'gamma': 1e-05, 'kernel': 'sigmoid'}\n",
      "0.131 (+/-0.050) for {'C': 0.001, 'kernel': 'linear'}\n",
      "0.851 (+/-0.092) for {'C': 0.1, 'kernel': 'linear'}\n",
      "0.851 (+/-0.092) for {'C': 0.1, 'kernel': 'linear'}\n",
      "0.987 (+/-0.052) for {'C': 10, 'kernel': 'linear'}\n",
      "0.987 (+/-0.052) for {'C': 25, 'kernel': 'linear'}\n",
      "0.987 (+/-0.052) for {'C': 50, 'kernel': 'linear'}\n",
      "0.987 (+/-0.052) for {'C': 100, 'kernel': 'linear'}\n",
      "0.987 (+/-0.052) for {'C': 1000, 'kernel': 'linear'}\n",
      "\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 25, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.058 (+/-0.005) for {'C': 0.001, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.058 (+/-0.005) for {'C': 0.001, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.058 (+/-0.005) for {'C': 0.001, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.058 (+/-0.005) for {'C': 0.001, 'gamma': 1e-05, 'kernel': 'rbf'}\n",
      "0.287 (+/-0.028) for {'C': 0.1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.173 (+/-0.016) for {'C': 0.1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.058 (+/-0.005) for {'C': 0.1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.058 (+/-0.005) for {'C': 0.1, 'gamma': 1e-05, 'kernel': 'rbf'}\n",
      "0.287 (+/-0.028) for {'C': 0.1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.173 (+/-0.016) for {'C': 0.1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.058 (+/-0.005) for {'C': 0.1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.058 (+/-0.005) for {'C': 0.1, 'gamma': 1e-05, 'kernel': 'rbf'}\n",
      "0.948 (+/-0.055) for {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.644 (+/-0.070) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.300 (+/-0.031) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.173 (+/-0.016) for {'C': 10, 'gamma': 1e-05, 'kernel': 'rbf'}\n",
      "1.000 (+/-0.000) for {'C': 25, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.819 (+/-0.086) for {'C': 25, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.402 (+/-0.031) for {'C': 25, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.173 (+/-0.016) for {'C': 25, 'gamma': 1e-05, 'kernel': 'rbf'}\n",
      "1.000 (+/-0.000) for {'C': 50, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.860 (+/-0.089) for {'C': 50, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.564 (+/-0.044) for {'C': 50, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.177 (+/-0.021) for {'C': 50, 'gamma': 1e-05, 'kernel': 'rbf'}\n",
      "1.000 (+/-0.000) for {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.948 (+/-0.055) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.644 (+/-0.070) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.301 (+/-0.031) for {'C': 100, 'gamma': 1e-05, 'kernel': 'rbf'}\n",
      "1.000 (+/-0.000) for {'C': 1000, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.988 (+/-0.050) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.948 (+/-0.055) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.644 (+/-0.070) for {'C': 1000, 'gamma': 1e-05, 'kernel': 'rbf'}\n",
      "0.058 (+/-0.005) for {'C': 0.001, 'gamma': 0.01, 'kernel': 'sigmoid'}\n",
      "0.058 (+/-0.005) for {'C': 0.001, 'gamma': 0.001, 'kernel': 'sigmoid'}\n",
      "0.058 (+/-0.005) for {'C': 0.001, 'gamma': 0.0001, 'kernel': 'sigmoid'}\n",
      "0.058 (+/-0.005) for {'C': 0.001, 'gamma': 1e-05, 'kernel': 'sigmoid'}\n",
      "0.091 (+/-0.012) for {'C': 0.1, 'gamma': 0.01, 'kernel': 'sigmoid'}\n",
      "0.115 (+/-0.011) for {'C': 0.1, 'gamma': 0.001, 'kernel': 'sigmoid'}\n",
      "0.058 (+/-0.005) for {'C': 0.1, 'gamma': 0.0001, 'kernel': 'sigmoid'}\n",
      "0.058 (+/-0.005) for {'C': 0.1, 'gamma': 1e-05, 'kernel': 'sigmoid'}\n",
      "0.091 (+/-0.012) for {'C': 0.1, 'gamma': 0.01, 'kernel': 'sigmoid'}\n",
      "0.115 (+/-0.011) for {'C': 0.1, 'gamma': 0.001, 'kernel': 'sigmoid'}\n",
      "0.058 (+/-0.005) for {'C': 0.1, 'gamma': 0.0001, 'kernel': 'sigmoid'}\n",
      "0.058 (+/-0.005) for {'C': 0.1, 'gamma': 1e-05, 'kernel': 'sigmoid'}\n",
      "0.208 (+/-0.042) for {'C': 10, 'gamma': 0.01, 'kernel': 'sigmoid'}\n",
      "0.553 (+/-0.045) for {'C': 10, 'gamma': 0.001, 'kernel': 'sigmoid'}\n",
      "0.177 (+/-0.021) for {'C': 10, 'gamma': 0.0001, 'kernel': 'sigmoid'}\n",
      "0.115 (+/-0.011) for {'C': 10, 'gamma': 1e-05, 'kernel': 'sigmoid'}\n",
      "0.206 (+/-0.030) for {'C': 25, 'gamma': 0.01, 'kernel': 'sigmoid'}\n",
      "0.670 (+/-0.076) for {'C': 25, 'gamma': 0.001, 'kernel': 'sigmoid'}\n",
      "0.330 (+/-0.032) for {'C': 25, 'gamma': 0.0001, 'kernel': 'sigmoid'}\n",
      "0.115 (+/-0.011) for {'C': 25, 'gamma': 1e-05, 'kernel': 'sigmoid'}\n",
      "0.193 (+/-0.029) for {'C': 50, 'gamma': 0.01, 'kernel': 'sigmoid'}\n",
      "0.819 (+/-0.086) for {'C': 50, 'gamma': 0.001, 'kernel': 'sigmoid'}\n",
      "0.402 (+/-0.031) for {'C': 50, 'gamma': 0.0001, 'kernel': 'sigmoid'}\n",
      "0.173 (+/-0.016) for {'C': 50, 'gamma': 1e-05, 'kernel': 'sigmoid'}\n",
      "0.180 (+/-0.018) for {'C': 100, 'gamma': 0.01, 'kernel': 'sigmoid'}\n",
      "0.875 (+/-0.078) for {'C': 100, 'gamma': 0.001, 'kernel': 'sigmoid'}\n",
      "0.564 (+/-0.044) for {'C': 100, 'gamma': 0.0001, 'kernel': 'sigmoid'}\n",
      "0.177 (+/-0.021) for {'C': 100, 'gamma': 1e-05, 'kernel': 'sigmoid'}\n",
      "0.172 (+/-0.023) for {'C': 1000, 'gamma': 0.01, 'kernel': 'sigmoid'}\n",
      "0.988 (+/-0.050) for {'C': 1000, 'gamma': 0.001, 'kernel': 'sigmoid'}\n",
      "0.863 (+/-0.082) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'sigmoid'}\n",
      "0.564 (+/-0.044) for {'C': 1000, 'gamma': 1e-05, 'kernel': 'sigmoid'}\n",
      "0.177 (+/-0.021) for {'C': 0.001, 'kernel': 'linear'}\n",
      "0.863 (+/-0.082) for {'C': 0.1, 'kernel': 'linear'}\n",
      "0.863 (+/-0.082) for {'C': 0.1, 'kernel': 'linear'}\n",
      "0.988 (+/-0.050) for {'C': 10, 'kernel': 'linear'}\n",
      "0.988 (+/-0.050) for {'C': 25, 'kernel': 'linear'}\n",
      "0.988 (+/-0.050) for {'C': 50, 'kernel': 'linear'}\n",
      "0.988 (+/-0.050) for {'C': 100, 'kernel': 'linear'}\n",
      "0.988 (+/-0.050) for {'C': 1000, 'kernel': 'linear'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-2, 1e-3, 1e-4, 1e-5],\n",
    "                     'C': [0.001, 0.10, 0.1, 10, 25, 50, 100, 1000]},\n",
    "                    {'kernel': ['sigmoid'], 'gamma': [1e-2, 1e-3, 1e-4, 1e-5],\n",
    "                     'C': [0.001, 0.10, 0.1, 10, 25, 50, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [0.001, 0.10, 0.1, 10, 25, 50, 100, 1000]}\n",
    "                   ]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(SVC(C=1), tuned_parameters, cv=5,\n",
    "                       scoring='%s_macro' % score)\n",
    "    clf.fit(data, target)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'gamma': 0.1}"
      ]
     },
     "execution_count": 894,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#svc_param_selection(data, target, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth:     10 | mean(train_error):  0.0326 | mean(val_error): 0.1955 | f1_score: 0.8022718720729038\n"
     ]
    }
   ],
   "source": [
    "# Training your support vector machine classifier, and print out your validation(or cross-validation) error.\n",
    "# Save your own model\n",
    "# Your Code Here\n",
    "#Create Pipeline\n",
    "standardizer = StandardScaler()\n",
    "data = df_train.drop(['feature1_a'],axis = 1,inplace = False)\n",
    "#Create K-fold cross-validation\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "train_errors = []\n",
    "val_errors = []\n",
    "for train_index, val_index in kf.split(data, target):       \n",
    "    #split data\n",
    "    X_train, X_val = data.iloc[train_index], data.iloc[val_index]\n",
    "    y_train, y_val = target.iloc[train_index], target.iloc[val_index]\n",
    "        \n",
    "    #instantaite model\n",
    "    #Create a pipleline that standardized, then runs logistic regression\n",
    "    pipeline = make_pipeline(standardizer,\n",
    "                            SVC(C=25, gamma=0.01, kernel='rbf'))\n",
    "        \n",
    "    #calculate errors\n",
    "    train_error, val_error, f1score = calc_metrics(X_train, y_train, X_val, y_val, pipeline)\n",
    "        \n",
    "    #append tp appropriate list\n",
    "    train_errors.append(train_error)\n",
    "    val_errors.append(val_error)\n",
    "    \n",
    "#generate report\n",
    "print('depth: {:6} | mean(train_error): {:7} | mean(val_error): {} | f1_score: {}'.\n",
    "        format(depth,\n",
    "               round(np.mean(train_errors),4),\n",
    "               round(np.mean(val_errors),4),\n",
    "               f1score\n",
    "                ))\n",
    "# End Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Option) Other Classifiers.\n",
    "Train and validate other classifiers by your own manner.\n",
    "> <b> If you need, you can import other models only in this cell, only in scikit-learn. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need additional packages, import your own packages below.\n",
    "# Your Code Here\n",
    "\n",
    "# End Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit your prediction on the test data.\n",
    "\n",
    "* Select your model and explain it briefly.\n",
    "* You should read <b>\"test.csv\"</b>.\n",
    "* Prerdict your model in array form.\n",
    "* Prediction example <br>\n",
    "[2, 6, 14, 8, $\\cdots$]\n",
    "* We will rank your result by <b>F1 metric(with 'macro' option)</b>.\n",
    "* <b> If you don't submit prediction file or submit it in wrong format, you can't get the point for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain your final model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset.\n",
    "# Your Code Here\n",
    "df_test = pd.read_csv('./data/test.csv')\n",
    "# End Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict target class\n",
    "# Make variable \"my_answer\", type of array, and fill this array with your class predictions.\n",
    "# Modify file name into your student number and your name.\n",
    "# Your Code Here\n",
    "my_answer = []\n",
    "\n",
    "#one-hot\n",
    "df_test_onehot = pd.get_dummies(df_test.drop(columns=['feature2','feature4','featrure6']))\n",
    "df_test = pd.concat([df_test,df_test_onehot], axis=1)\n",
    "df_test.drop(['feature1','feature3','feature5'], axis = 1, inplace = True)\n",
    "\n",
    "logreg_test = logreg.predict(df_test)\n",
    "\n",
    "file_name = \"HW2_2016320120_정소영.csv\"\n",
    "# End Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 6, 6, ..., 1, 6, 2], dtype=int64)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section is for saving predicted answers. DO NOT MODIFY.\n",
    "pd.Series(my_answer).to_csv(\"./data/\" + file_name, header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
